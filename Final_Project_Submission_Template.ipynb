{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final Project Submission Template",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9VDfC-ie19P"
      },
      "source": [
        "# Machine Learning algorithms for the detection and localization of surgically resectable cancers\n",
        "**Project mentor:** Joshua Popp\n",
        "\n",
        "**Members:** Chris Wilhelm <cwilhel8@jh.edu>, Jayden Kunwar <jkunwar1@jhu.edu>, <br> Bryce Thalheimer <bthalhe1@jh.edu>, Arthur Beyer <cbeyer4@jhu.edu>\n",
        "\n",
        "**GitHub Repository:** <br>\n",
        "<https://github.com/ChrisWilhelm/MachineLearningFinalProject>\n",
        "\n",
        "**Inspiration for Project:** <br>\n",
        "https://www.science.org/doi/10.1126/science.aar3247?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub%20%200pubmed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqwI3PT-hBJo"
      },
      "source": [
        "# Outline and Deliverables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7Af6y48e7HI"
      },
      "source": [
        "### Uncompleted Deliverables\n",
        "1. \"Expect to complete #1\": Discussed data pre-processing with Josh and concluded that it wasn't necessary for our dataset\n",
        "2. ...\n",
        "\n",
        "\n",
        "### Completed Deliverables\n",
        "1. \"Must complete #1\": We discuss our logistic regression [in \"Models\" below](#scrollTo=PqB48IF9kMBf).\n",
        "2. \"Must complete #2\": We discuss training our neural network [in \"Models\" below](#scrollTo=PqB48IF9kMBf).\n",
        "3. \"Must complete #3\": We discuss AdaBoost [in \"Models\" below](#scrollTo=PqB48IF9kMBf).\n",
        "4. \"Expect to complete #2\": We discussed interpretability...\n",
        "5. \"Expect to complete #3\": Multi-Class Classification of Cancer Type\n",
        "6. \"Would like to complete #1\": High Specificity\n",
        "6. \"Would like to complete #1\": Additional Data\n",
        "6. \"Would like to complete #1\": Cancer Localization\n",
        "\n",
        "\n",
        "### Additional Deliverables\n",
        "(example) 1. We decided to add a second baseline using the published model from this paper. We discuss this [in \"Baselines\" below](#scrollTo=oMyqHUa0jUw7&line=5&uniqifier=1).\n",
        "2. ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eiq2aSauhSsS"
      },
      "source": [
        "# Preliminaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtWkhiIPfOfK"
      },
      "source": [
        "## What problem were you trying to solve or understand?\n",
        "\n",
        "\n",
        "What are the real-world implications of this data and task?\n",
        "\n",
        "How is this problem similar to others weâ€™ve seen in lectures, breakouts, and homeworks?\n",
        "\n",
        "What makes this problem unique?\n",
        "\n",
        "What ethical implications does this problem have?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFq-_D0khnhh"
      },
      "source": [
        "## Dataset(s)\n",
        "\n",
        "Describe the dataset(s) you used.\n",
        "\n",
        "How were they collected?\n",
        "\n",
        "Why did you choose them?\n",
        "\n",
        "How many examples in each?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lOicoBYif7g"
      },
      "source": [
        "# Load your data and print 2-3 examples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XN1fYEfGidiD"
      },
      "source": [
        "## Pre-processing\n",
        "\n",
        "What features did you use or choose not to use? Why?\n",
        "\n",
        "If you have categorical labels, were your datasets class-balanced?\n",
        "\n",
        "How did you deal with missing data? What about outliers?\n",
        "\n",
        "What approach(es) did you use to pre-process your data? Why?\n",
        "\n",
        "Are your features continuous or categorical? How do you treat these features differently?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEuKEzM5ipag"
      },
      "source": [
        "# For those same examples above, what do they look like after being pre-processed?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cDLEwhAx0gP"
      },
      "source": [
        "# Visualize the distribution of your data before and after pre-processing.\n",
        "#   You may borrow from how we visualized data in the Lab homeworks."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tASjmmtjiwvu"
      },
      "source": [
        "# Models and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlrwR9E1hnQ3"
      },
      "source": [
        "## Experimental Setup\n",
        "\n",
        "How did you evaluate your methods? Why is that a reasonable evaluation metric for the task?\n",
        "\n",
        "What did you use for your loss function to train your models? Did you try multiple loss functions? Why or why not?\n",
        "\n",
        "How did you split your data into train and test sets? Why?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUNxC358jPDr"
      },
      "source": [
        "# Code for loss functions, evaluation metrics or link to Git repo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMyqHUa0jUw7"
      },
      "source": [
        "## Baselines \n",
        "\n",
        "What baselines did you compare against? Why are these reasonable?\n",
        "\n",
        "Did you look at related work to contextualize how others methods or baselines have performed on this dataset/task? If so, how did those methods do?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqB48IF9kMBf"
      },
      "source": [
        "## Methods\n",
        "\n",
        "What methods did you choose? Why did you choose them?\n",
        "\n",
        "How did you train these methods, and how did you evaluate them? Why?\n",
        "\n",
        "Which methods were easy/difficult to implement and train? Why?\n",
        "\n",
        "For each method, what hyperparameters did you evaluate? How sensitive was your model's performance to different hyperparameter settings?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ma4JoDzar6xU"
      },
      "source": [
        "# Code for training models, or link to your Git repository"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RO_kP1fmkWWk"
      },
      "source": [
        "# Show plots of how these models performed during training.\n",
        "#  For example, plot train loss and train accuracy (or other evaluation metric) on the y-axis,\n",
        "#  with number of iterations or number of examples on the x-axis."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Zdp4_H-kx8H"
      },
      "source": [
        "## Results\n",
        "\n",
        "Show tables comparing your methods to the baselines.\n",
        "\n",
        "What about these results surprised you? Why?\n",
        "\n",
        "Did your models over- or under-fit? How can you tell? What did you do to address these issues?\n",
        "\n",
        "What does the evaluation of your trained models tell you about your data? How do you expect these models might behave differently on different data?  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bS2sjfbglG_V"
      },
      "source": [
        "# Show plots or visualizations of your evaluation metric(s) on the train and test sets.\n",
        "#   What do these plots show about over- or under-fitting?\n",
        "#   You may borrow from how we visualized results in the Lab homeworks.\n",
        "#   Are there aspects of your results that are difficult to visualize? Why?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59EbS1GilSQ_"
      },
      "source": [
        "# Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugJXhZKNlUT4"
      },
      "source": [
        "## What you've learned\n",
        "\n",
        "*Note: you don't have to answer all of these, and you can answer other questions if you'd like. We just want you to demonstrate what you've learned from the project.*\n",
        "\n",
        "What concepts from lecture/breakout were most relevant to your project? How so?\n",
        "\n",
        "What aspects of your project did you find most surprising?\n",
        "\n",
        "What lessons did you take from this project that you want to remember for the next ML project you work on? Do you think those lessons would transfer to other datasets and/or models? Why or why not?\n",
        "\n",
        "What was the most helpful feedback you received during your presentation? Why?\n",
        "\n",
        "If you had two more weeks to work on this project, what would you do next? Why?"
      ]
    }
  ]
}